<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
  <link rel="mask-icon" href="/favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qiufengyu.github.io","root":"/","scheme":"Gemini","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Matrix factorization (MF) is often used in building recommender systems. The extension of it, tensor factorization has come to us.">
<meta property="og:type" content="article">
<meta property="og:title" content="Matrix and Tensor Factorization Techniques">
<meta property="og:url" content="http://qiufengyu.github.io/2017/matrix-and-tensor-factorization/">
<meta property="og:site_name" content="Heaven&#39;s Wall">
<meta property="og:description" content="Matrix factorization (MF) is often used in building recommender systems. The extension of it, tensor factorization has come to us.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://qiufengyu.github.io/2017/matrix-and-tensor-factorization/tensor1.PNG">
<meta property="og:image" content="http://qiufengyu.github.io/2017/matrix-and-tensor-factorization/tensor2.PNG">
<meta property="og:image" content="http://qiufengyu.github.io/2017/matrix-and-tensor-factorization/tensor3.PNG">
<meta property="og:image" content="http://qiufengyu.github.io/2017/matrix-and-tensor-factorization/tensor4.PNG">
<meta property="article:published_time" content="2017-04-22T19:12:17.000Z">
<meta property="article:modified_time" content="2020-02-23T17:24:17.539Z">
<meta property="article:author" content="Godfray Qiu">
<meta property="article:tag" content="Recommender Systems">
<meta property="article:tag" content="Matrix Factorization">
<meta property="article:tag" content="Tensor Factorization">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://qiufengyu.github.io/2017/matrix-and-tensor-factorization/tensor1.PNG">

<link rel="canonical" href="http://qiufengyu.github.io/2017/matrix-and-tensor-factorization/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Matrix and Tensor Factorization Techniques | Heaven's Wall</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta custom-logo">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Heaven's Wall</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">落尽红樱君不见，轻绘梨花泪沾衣</p>
      <a>
        <img class="custom-logo-image" src="/images/custom-logo.jpg" alt="Heaven's Wall">
      </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://qiufengyu.github.io/2017/matrix-and-tensor-factorization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Godfray Qiu">
      <meta itemprop="description" content="清纯佳人白玫瑰，高冷酸假">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heaven's Wall">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Matrix and Tensor Factorization Techniques
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-04-22 19:12:17" itemprop="dateCreated datePublished" datetime="2017-04-22T19:12:17+00:00">2017-04-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-23 17:24:17" itemprop="dateModified" datetime="2020-02-23T17:24:17+00:00">2020-02-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Recommendation/" itemprop="url" rel="index"><span itemprop="name">Recommendation</span></a>
                </span>
            </span>

          
            <div class="post-description">Matrix factorization (MF) is often used in building recommender systems. The extension of it, tensor factorization has come to us.</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h2><h3 id="Eigenvalue-Decomposition"><a href="#Eigenvalue-Decomposition" class="headerlink" title="Eigenvalue Decomposition"></a>Eigenvalue Decomposition</h3><p>矩阵的特征值与特征向量 $A\vec e = \lambda \vec e$<br>从而有 $AE = E \Lambda$<br>那么矩阵 $A$ 可分解为 $A = E\Lambda E^{-1}$</p>
<h3 id="Non-negative-Matrix-Factorization-NMF"><a href="#Non-negative-Matrix-Factorization-NMF" class="headerlink" title="Non-negative Matrix Factorization (NMF)"></a>Non-negative Matrix Factorization (NMF)</h3><p>矩阵 $A$ 分解为 $UV$ 的形式，$A, U, V$ 均不含负值元素</p>
<p>最小化：$||A-UV|| = \sum_{k=1}^K(a_{mn}-u_{mk}v_{kn})^2$</p>
<p>更新：$U \leftarrow U\frac{AV^T}{AVV^T}$，$V \leftarrow V\frac{U^TA}{U^TUV}$</p>
<p>注意：最小化问题仅仅对单一的 $U$ 或 $V$ 是凸的，因此，只能达到局部最优的近似解。</p>
<h3 id="Latent-Semantic-Indexing"><a href="#Latent-Semantic-Indexing" class="headerlink" title="Latent Semantic Indexing"></a>Latent Semantic Indexing</h3><p>基于 SVD 方法，$A_{I_1 \times I_2} = U_{I_1\times I_1} \cdot S_{I_1 \times I_2} \cdot V^T_{I_2\times I_2}$，这里的 $S$ 是一个对角矩阵，其元素为 $A$ 的非负奇异值按从大到小的排列。</p>
<h3 id="Probabilistic-Latent-Semantic-Indexing"><a href="#Probabilistic-Latent-Semantic-Indexing" class="headerlink" title="Probabilistic Latent Semantic Indexing"></a>Probabilistic Latent Semantic Indexing</h3><p>LSI 的概率版本，原本多用于文档-主题-单词模型。用矩阵 $A$ 中的 $a_{ij}$ 即为概率 $P(d_i, w_j)$ 的观察值， $P(d_i, w_j)$ 的生成过程首先以某种概率 $P(T_l)$ 选择一个隐藏因子 (Latent Factor) $T_l$，接着根据条件概率 $P(d_i|T_l)$ 和 $P(w_j |T_l)$ 采样出文档（用户）和单词（商品）的概率。</p>
<p>矩阵分解形式：$R = U_k \times S_k \times V^T_k$，$U_k$ 的第 $(i, l)$ 元素即为概率 $P(d_i | T_l)$，对角矩阵 $S_k$ 的第 $l$ 个对角元素为 $P(T_l)$，$V_k$ 的第 $(j,l)$ 元素即为 $P(w_j |T_l)$ </p>
<p>从而，$P(d_i, w_j) = \sum_{l=1}^kP(d_i|T_l) P(T_l) P(w_j|T_l)$</p>
<h3 id="CUR-Matrix-Decomposition"><a href="#CUR-Matrix-Decomposition" class="headerlink" title="CUR Matrix Decomposition"></a>CUR Matrix Decomposition</h3><p>矩阵 $A_{m \times n} = C_{m \times c}U_{c \times c}R_{c \times n}$，其中，</p>
<ul>
<li>矩阵 $C$ 为矩阵 $A$ 中取出的 $c$ 列；</li>
</ul>
<ul>
<li>矩阵 $R$ 为矩阵 $A$ 中取出的 $c$ 行；</li>
<li>矩阵 $U$ 的构造按照如下方式：<ul>
<li>取矩阵 $C$ 和 $U$ 的交集，记为 $W$</li>
<li>对 $W$ 进行SVD，$W = \overline{U}\overline{S}\overline{V}^T$，同时计算出 $\overline{S}$ 的 广义逆 $\overline{S}^+$ （把 $\overline{S}$ 中的非 $0$ 元素变为其倒数，并转置）</li>
<li>矩阵 $U = \overline{V}(\overline{S}^+)^2\overline{U}^{T}$</li>
<li>注：其实是计算 $W$ 的伪逆过程，但是维基百科给出的用 SVD 方法求伪逆过程， $\overline{S}$ 没有平方</li>
</ul>
</li>
</ul>
<p>为了提高 CUR 分解的准确性，对于 $C$ 和 $R$ 的采样需根据概率：$P(i) = \sum_{j=1}^n \frac{a_{i,j}^2}{|A|_F}$，其中 $|A|_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^n|a_{i,j}|^2}$，选出的列或者行需要规范化，除以 $\sqrt{c \times P(i)}$</p>
<p>CUR 比 SVD 的运行效率高，因为它只有矩阵 $U​$ 比较密集，而通常 $U​$ 的规模不会设置的很大。但是其精度会差一些。</p>
<h3 id="Cholesky-Decomposition"><a href="#Cholesky-Decomposition" class="headerlink" title="Cholesky Decomposition"></a>Cholesky Decomposition</h3><p>要求矩阵 $A$ 为正定矩阵，那么 $A$ 可分解为 $L\overline{L}^T$ 的形式，$L$ 为一下三角矩阵，$\overline{L}^T$ 为 $L$ 的共轭转置。</p>
<h2 id="Tensor-Factorization"><a href="#Tensor-Factorization" class="headerlink" title="Tensor Factorization"></a>Tensor Factorization</h2><h3 id="Tensor-背景知识"><a href="#Tensor-背景知识" class="headerlink" title="Tensor 背景知识"></a>Tensor 背景知识</h3><p>以三维为例，在矩阵的基础上增加了一维，可看作多个二维矩阵的堆叠。用 $I_1$ 表示行数，$I_2$ 表示列数，$I_3$ 表示堆叠的厚度。一个 3 阶张量 $ \mathcal{A} \in \mathbb{R}^{I_1\times I_2 \times I_3}$ 有三种展开方式：</p>
<ol>
<li>$A_1 \in \mathbb{R}^{I_1\times(I_2 \times I_3)}$</li>
<li>$A_2 \in \mathbb{R}^{I_2\times(I_1 \times I_3)}$</li>
<li>$A_3 \in \mathbb{R}^{(I_1 \times I_2)\times I_3}$</li>
</ol>
<p><img src="tensor1.PNG" alt="TensorUnfolding"></p>
<p>例子：</p>
<p><img src="tensor2.PNG" alt="TensorUnfoldingMode"></p>
<p>在此基础上，定义模积的概念：N-阶张量 $\mathcal{A} \in \mathbb{R}^{I_1 \times \cdots \times I_N}$ 与矩阵 $U \in \mathbb{R}^{J_n \times I_n}$ 的 $n$-模积是一个 $I_1 \times I_2 \times \cdots \times I_{n-1}\times J_n \times I_{n+1} \times \cdots \times I_N$ 的张量，记为 $\mathcal{A} \times_{n} U$，即：<br>$$<br>(\mathcal{A} \times_n U)_{i_1i_2\cdots j_n i_{n+1}\cdots i_N} = \sum_{i_n}a_{i_1i_2\cdots i_{n-1}i_ni_{n+1}\cdots i_Nu_{j_n, i_n}}<br>$$<br>从而，一般的 SVD 二维矩阵分解可以表示为：$F = S \times_1 U^{(1)} \times_2 U^{(2)}$。扩展为三维张量形式：$\mathcal{A}= S \times_1 U^{(1)} \times_2 U^{(2)} \times_3 U^{(3)}$，其中的 $U^{(k)}$ 对应着 $k$-模展开矩阵列向量的正交扩充。</p>
<h3 id="HOSVD"><a href="#HOSVD" class="headerlink" title="HOSVD"></a>HOSVD</h3><p>HOSVD是 Tucker Decomposition 的一个变种，把一个张量分解为一系列矩阵和一个规模较小的核心张量。以社交网络标签系统为例，三维张量 $\mathcal{A} \in \mathbb{R}^{|U| \times |I| \times |T|}$，在观察集 $Y$ 中当用户 $u$ 关于物品 $i$ 打上标签 $t$ 时，$a_{u,i,t} =1$ ，否则 $a_{u,i,t} =0$。那么 $\mathcal{A}$ 有分解形式：$\mathcal{\hat{A}} = \hat{C} \times_u \hat{U} \times_i \hat{I} \times_t \hat{T}$。</p>
<p><img src="tensor3.PNG" alt="HOSVD"></p>
<p>分解的优化目标参数：$\theta=(\hat{C}, \hat{U}, \hat{I}, \hat{T})$</p>
<p>优化目标：$\arg\min\limits_{\hat{\theta}}\sum\limits_{(u,i,t) \in Y}(\hat{a}_{u,i,t}-a_{u,i,t})^2$</p>
<h3 id="AlsHOSVD"><a href="#AlsHOSVD" class="headerlink" title="AlsHOSVD"></a>AlsHOSVD</h3><p>ALS 表示 Alternating least squares，使用各个模展开矩阵的前 $k$ 个左奇异向量，具体流程如下：</p>
<p><strong>输入</strong>：张量 $\mathcal{A}$</p>
<p><strong>输出</strong>：近似张量 $\mathcal{\hat{A}}$ 的每一维模展开的前 $k_U, k_I, k_T$ 个左特征向量</p>
<ol>
<li>初始化核心张量 $\mathcal{C}$ 和 $A_1, A_2, A_3$ 的左奇异向量矩阵 $U^{(1)}, U^{(2)}, U^{(3)}$ </li>
<li><strong>repeat</strong></li>
<li>​    $\mathcal{C} = \mathcal{A} \times_1 U_{k_U}^{(1)^T} \times_2 U_{k_I}^{(2)^T} \times_3 U_{k_T}^{(3)^T}$</li>
<li>​    $\mathcal{\hat{A}} = \mathcal{C} \times_1 U_{k_U}^{(1)} \times_2 U_{k_I}^{(2)} \times_3 U_{k_T}^{(3)}$</li>
<li>​    $U_{k_U}^{(1)} \leftarrow $ $A_1$ 的前 $k_U$ 个左奇异向量</li>
<li>​    $U_{k_I}^{(2)} \leftarrow $ $A_2$ 的前 $k_I$ 个左奇异向量</li>
<li>​    $U_{k_T}^{(3)} \leftarrow $ $A_3$ 的前 $k_T$ 个左奇异向量</li>
<li><strong>until</strong> $|\mathcal{A} - \mathcal{\hat{A}}|^2$ 不再变化或达到预设的迭代次数</li>
<li><strong>return</strong> $\mathcal{C}, U_{k_U}^{(1)}, U_{k_I}^{(2)}, U_{k_T}^{(3)}$</li>
</ol>
<h3 id="Parallel-Factor-Analysis"><a href="#Parallel-Factor-Analysis" class="headerlink" title="Parallel Factor Analysis"></a>Parallel Factor Analysis</h3><p>PARAFAC，也称 Canonical Decomposition，是传统 Tucker Decomposition 的一种特例形式。这里的核心张量 $\mathcal{C}$ 只有对角元素为 $1$。图 (a) 为 HOSVD 的分解模型，图 (b) 即为 PARAFAC 的分解模型。</p>
<p><img src="tensor4.PNG" alt="PARAFAC and TD"></p>
<h3 id="Pairwise-Interaction-Tensor-Factorization"><a href="#Pairwise-Interaction-Tensor-Factorization" class="headerlink" title="Pairwise Interaction Tensor Factorization"></a>Pairwise Interaction Tensor Factorization</h3><p>PITF，不同于传统 TD 和 PARAFAC，从二维的角度实现拟合。<br>$$<br>\hat{a}_{u,r,t} = \sum\limits_{f}^k\hat{u}_{u,f} \cdot \hat{t}_{t,f}^U + \sum\limits_{f}^k\hat{i}_{i,f} \cdot \hat{t}_{t,f}^I<br>$$<br>因此，该模型有四个参数：$\hat{U}\in \mathbb{I}^{|U| \times k}, \hat{I}\in \mathbb{I}^{|I| \times k}, \hat{T}^U\in \mathbb{I}^{|T| \times k}, \hat{T}^I\in \mathbb{I}^{|T| \times k}$</p>
<h3 id="PCLAF-and-RPCLAF"><a href="#PCLAF-and-RPCLAF" class="headerlink" title="PCLAF and RPCLAF"></a>PCLAF and RPCLAF</h3><p>通过隐藏因子对齐的方式进行拟合。</p>
<p>详细内容参见：Zheng, V., Zheng, Y., Xie, X., Yang, Q.: Towards mobile intelligence: learning from GPS history data for collaborative recommendation. Artif. Intell. 184–185, 17–37 (2012)</p>
<h3 id="Limitations-and-Extensions"><a href="#Limitations-and-Extensions" class="headerlink" title="Limitations and Extensions"></a>Limitations and Extensions</h3><p>在推荐系统问题中，矩阵 SVD 分解的矩阵已经很稀疏，那么张量分解对应的张量会更加稀疏。为此，可以用 “kernel trick” 将数据空间映射到高维的方法。我们在计算做奇异向量时，用的是矩阵的内积，即 $FF^T$ 的形式求得向量，如果通过 kernel 函数替换普通的内积，就可以完成映射，并且不会增加额外的计算量，与 kernel SVD 方法类似。</p>
<p>HOSVD，PARAFAC 法对应的优化目标都不是凸 (convex) 的，所以会有多个局部最优点。在实际数据集上，有资料显示 HOSVD 要更为稳定。</p>
<p>其他扩展：</p>
<ol>
<li><p>HOSVD + Content-Based Method：Nanopoulos, A., Rafailidis, D., Symeonidis, P., Manolopoulos, Y.: Musicbox: personalized music recommendation based on cubic analysis of social tags. IEEE Trans. Audio Speech Lang. Process. 18(2), 407–412 (2010)</p>
<p>数据集是 Last.fm</p>
</li>
<li><p>HOSVD + Clustering Method：Symeonidis, P.: ClustHOSVD: item recommendation by combining semantically enhanced tag clustering with tensor HOSVD. IEEE Syst. Man Cybern. (2015)</p>
<p>大致思路是采用 tf-idf 加上标签的语义信息，做 tag 的 clustering，然后根据每个 cluster  的中心建立标签这个维度。</p>
</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Recommender-Systems/" rel="tag"># Recommender Systems</a>
              <a href="/tags/Matrix-Factorization/" rel="tag"># Matrix Factorization</a>
              <a href="/tags/Tensor-Factorization/" rel="tag"># Tensor Factorization</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2016/code-recommendation/" rel="prev" title="Code Recommendation Project">
      <i class="fa fa-chevron-left"></i> Code Recommendation Project
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/reading1/" rel="next" title="跨领域推荐系统，Item Silk Road">
      跨领域推荐系统，Item Silk Road <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix-Factorization"><span class="nav-number">1.</span> <span class="nav-text">Matrix Factorization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Eigenvalue-Decomposition"><span class="nav-number">1.1.</span> <span class="nav-text">Eigenvalue Decomposition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Non-negative-Matrix-Factorization-NMF"><span class="nav-number">1.2.</span> <span class="nav-text">Non-negative Matrix Factorization (NMF)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Latent-Semantic-Indexing"><span class="nav-number">1.3.</span> <span class="nav-text">Latent Semantic Indexing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Probabilistic-Latent-Semantic-Indexing"><span class="nav-number">1.4.</span> <span class="nav-text">Probabilistic Latent Semantic Indexing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CUR-Matrix-Decomposition"><span class="nav-number">1.5.</span> <span class="nav-text">CUR Matrix Decomposition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cholesky-Decomposition"><span class="nav-number">1.6.</span> <span class="nav-text">Cholesky Decomposition</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor-Factorization"><span class="nav-number">2.</span> <span class="nav-text">Tensor Factorization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor-背景知识"><span class="nav-number">2.1.</span> <span class="nav-text">Tensor 背景知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HOSVD"><span class="nav-number">2.2.</span> <span class="nav-text">HOSVD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AlsHOSVD"><span class="nav-number">2.3.</span> <span class="nav-text">AlsHOSVD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parallel-Factor-Analysis"><span class="nav-number">2.4.</span> <span class="nav-text">Parallel Factor Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pairwise-Interaction-Tensor-Factorization"><span class="nav-number">2.5.</span> <span class="nav-text">Pairwise Interaction Tensor Factorization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PCLAF-and-RPCLAF"><span class="nav-number">2.6.</span> <span class="nav-text">PCLAF and RPCLAF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Limitations-and-Extensions"><span class="nav-number">2.7.</span> <span class="nav-text">Limitations and Extensions</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Godfray Qiu"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Godfray Qiu</p>
  <div class="site-description" itemprop="description">清纯佳人白玫瑰，高冷酸假</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qiufengyu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qiufengyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:mrqiufengyu@gmail.com" title="E-Mail → mailto:mrqiufengyu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/237533031" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;237533031" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://space.bilibili.com/6625805" title="Bilibili → http:&#x2F;&#x2F;space.bilibili.com&#x2F;6625805" rel="noopener" target="_blank"><i class="fa fa-fw fa-bomb"></i>Bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/GodfrayQiu" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;GodfrayQiu" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/qiufengyu" title="Facebook → https:&#x2F;&#x2F;www.facebook.com&#x2F;qiufengyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i>Facebook</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://instagram.com/godfrayqiu" title="Instagram → http:&#x2F;&#x2F;instagram.com&#x2F;godfrayqiu" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/qiufengyu" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;qiufengyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i>LinkedIn</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2014 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Godfray Qiu</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  











<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes : 0,
          toolbar  : 0,
          statusbar: 0,
          pagemode : 'thumbs',
          view     : 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '1220px'
      });
    });
  }, window.PDFObject);
}
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
